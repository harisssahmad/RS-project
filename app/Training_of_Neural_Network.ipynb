{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TMxjOBLWosFA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import os\n",
        "import signal\n",
        "import sys\n",
        "import pickle\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AnimeRecommendationSystem:\n",
        "    def __init__(self, model_path='anime_recommendation_model.h5',\n",
        "                 checkpoint_path='model_checkpoint.h5',\n",
        "                 training_state_path='training_state.json'):\n",
        "        self.model_path = model_path\n",
        "        self.checkpoint_path = checkpoint_path\n",
        "        self.training_state_path = training_state_path\n",
        "        self.model = None\n",
        "        self.training_interrupted = False\n",
        "        self.current_epoch = 0\n",
        "        self.history = {'loss': [], 'val_loss': [], 'mae': [], 'val_mae': []}\n",
        "\n",
        "        # Set up signal handler for graceful shutdown\n",
        "        signal.signal(signal.SIGINT, self.signal_handler)\n",
        "\n",
        "    def signal_handler(self, signum, frame):\n",
        "        \"\"\"Handle Ctrl+C gracefully by saving the model and training state\"\"\"\n",
        "        print(\"\\n\\nğŸ›‘ Training interrupted! Saving model and training state...\")\n",
        "        self.training_interrupted = True\n",
        "\n",
        "        if self.model is not None:\n",
        "            try:\n",
        "                # Save the current model\n",
        "                self.model.save(self.checkpoint_path)\n",
        "                print(f\"âœ… Model saved to {self.checkpoint_path}\")\n",
        "\n",
        "                # Save training state\n",
        "                self.save_training_state()\n",
        "                print(f\"âœ… Training state saved to {self.training_state_path}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Error saving model: {e}\")\n",
        "\n",
        "        print(\"ğŸ‘‹ Exiting gracefully...\")\n",
        "        sys.exit(0)\n",
        "\n",
        "    def save_training_state(self):\n",
        "        \"\"\"Save current training state to resume later\"\"\"\n",
        "        training_state = {\n",
        "            'current_epoch': self.current_epoch,\n",
        "            'history': self.history,\n",
        "            'interrupted': True\n",
        "        }\n",
        "\n",
        "        with open(self.training_state_path, 'w') as f:\n",
        "            json.dump(training_state, f, indent=2)\n",
        "\n",
        "    def load_training_state(self):\n",
        "        \"\"\"Load previous training state if it exists\"\"\"\n",
        "        if os.path.exists(self.training_state_path):\n",
        "            try:\n",
        "                with open(self.training_state_path, 'r') as f:\n",
        "                    state = json.load(f)\n",
        "                    self.current_epoch = state.get('current_epoch', 0)\n",
        "                    self.history = state.get('history', {'loss': [], 'val_loss': [], 'mae': [], 'val_mae': []})\n",
        "                    was_interrupted = state.get('interrupted', False)\n",
        "\n",
        "                if was_interrupted:\n",
        "                    print(f\"ğŸ“ Found interrupted training session at epoch {self.current_epoch}\")\n",
        "                    return True\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Error loading training state: {e}\")\n",
        "\n",
        "        return False\n",
        "\n",
        "    def load_or_create_model(self, num_users, num_anime, embedding_dim=128):\n",
        "        \"\"\"Load existing model or create a new one\"\"\"\n",
        "\n",
        "        # Check for interrupted training first\n",
        "        was_interrupted = self.load_training_state()\n",
        "\n",
        "        # Try to load the main model\n",
        "        if os.path.exists(self.model_path):\n",
        "            try:\n",
        "                self.model = load_model(self.model_path, custom_objects={'mse': MeanSquaredError()})\n",
        "                print(\"âœ… Main model loaded successfully!\")\n",
        "                return True, False  # loaded, not_newly_created\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Error loading main model: {e}\")\n",
        "\n",
        "        # Try to load checkpoint if interrupted training exists\n",
        "        if was_interrupted and os.path.exists(self.checkpoint_path):\n",
        "            try:\n",
        "                self.model = load_model(self.checkpoint_path, custom_objects={'mse': MeanSquaredError()})\n",
        "                print(\"âœ… Checkpoint model loaded successfully! Ready to resume training.\")\n",
        "                return True, True  # loaded, needs_training\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Error loading checkpoint: {e}\")\n",
        "\n",
        "        # Create new model if no existing model found\n",
        "        print(\"ğŸ”¨ Creating new model...\")\n",
        "        self.model = self.create_model(num_users, num_anime, embedding_dim)\n",
        "        return False, True  # not_loaded, needs_training\n",
        "\n",
        "    def create_model(self, num_users, num_anime, embedding_dim=128):\n",
        "        \"\"\"Create a new neural network model\"\"\"\n",
        "        user_input = tf.keras.layers.Input(shape=(1,), name='user_input')\n",
        "        anime_input = tf.keras.layers.Input(shape=(1,), name='anime_input')\n",
        "\n",
        "        user_embedding = tf.keras.layers.Embedding(\n",
        "            input_dim=num_users, output_dim=embedding_dim, name='user_embedding'\n",
        "        )(user_input)\n",
        "        anime_embedding = tf.keras.layers.Embedding(\n",
        "            input_dim=num_anime, output_dim=embedding_dim, name='anime_embedding'\n",
        "        )(anime_input)\n",
        "\n",
        "        user_vec = tf.keras.layers.Flatten()(user_embedding)\n",
        "        anime_vec = tf.keras.layers.Flatten()(anime_embedding)\n",
        "\n",
        "        concat = tf.keras.layers.Concatenate()([user_vec, anime_vec])\n",
        "        dense = tf.keras.layers.Dense(256, activation='relu')(concat)\n",
        "        dense = tf.keras.layers.Dropout(0.2)(dense)  # Added dropout for regularization\n",
        "        dense = tf.keras.layers.Dense(128, activation='relu')(dense)\n",
        "        dense = tf.keras.layers.Dropout(0.2)(dense)\n",
        "        output = tf.keras.layers.Dense(1, activation='linear')(dense)\n",
        "\n",
        "        model = tf.keras.models.Model(inputs=[user_input, anime_input], outputs=output)\n",
        "        model.compile(optimizer='adam', loss=MeanSquaredError(), metrics=['mae'])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def train_model(self, X_train, y_train, X_test, y_test, epochs=50, batch_size=64):\n",
        "        \"\"\"Train the model with resume capability\"\"\"\n",
        "\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not initialized. Call load_or_create_model first.\")\n",
        "\n",
        "        # Calculate remaining epochs if resuming\n",
        "        remaining_epochs = epochs - self.current_epoch\n",
        "        if remaining_epochs <= 0:\n",
        "            print(\"ğŸ¯ Training already completed!\")\n",
        "            return self.history\n",
        "\n",
        "        print(f\"ğŸš€ Starting training from epoch {self.current_epoch + 1}/{epochs}\")\n",
        "\n",
        "        # Callbacks\n",
        "        callbacks = [\n",
        "            # Save best model during training\n",
        "            ModelCheckpoint(\n",
        "                filepath=self.checkpoint_path,\n",
        "                monitor='val_loss',\n",
        "                save_best_only=True,\n",
        "                save_weights_only=False,\n",
        "                verbose=1\n",
        "            ),\n",
        "            # Early stopping\n",
        "            EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=5,\n",
        "                restore_best_weights=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "            # Custom callback to handle graceful interruption\n",
        "            GracefulInterruptCallback(self)\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            # Train the model\n",
        "            history = self.model.fit(\n",
        "                [X_train[:, 0], X_train[:, 1]],\n",
        "                y_train,\n",
        "                validation_data=([X_test[:, 0], X_test[:, 1]], y_test),\n",
        "                epochs=remaining_epochs,\n",
        "                batch_size=batch_size,\n",
        "                callbacks=callbacks,\n",
        "                verbose=1,\n",
        "                initial_epoch=self.current_epoch\n",
        "            )\n",
        "\n",
        "            # Update training history\n",
        "            for key in history.history:\n",
        "                if key in self.history:\n",
        "                    self.history[key].extend(history.history[key])\n",
        "                else:\n",
        "                    self.history[key] = history.history[key]\n",
        "\n",
        "            # Save the final model\n",
        "            if not self.training_interrupted:\n",
        "                self.model.save(self.model_path)\n",
        "                print(f\"âœ… Training completed! Model saved to {self.model_path}\")\n",
        "\n",
        "                # Clean up checkpoint and training state\n",
        "                self.cleanup_training_files()\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nğŸ›‘ Training interrupted by user!\")\n",
        "            self.signal_handler(signal.SIGINT, None)\n",
        "\n",
        "        return self.history\n",
        "\n",
        "    def cleanup_training_files(self):\n",
        "        \"\"\"Clean up temporary training files after successful completion\"\"\"\n",
        "        try:\n",
        "            if os.path.exists(self.checkpoint_path):\n",
        "                os.remove(self.checkpoint_path)\n",
        "                print(f\"ğŸ§¹ Cleaned up checkpoint: {self.checkpoint_path}\")\n",
        "\n",
        "            if os.path.exists(self.training_state_path):\n",
        "                os.remove(self.training_state_path)\n",
        "                print(f\"ğŸ§¹ Cleaned up training state: {self.training_state_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"âš ï¸ Error cleaning up files: {e}\")\n",
        "\n",
        "    def recommend_nn_based(self, user_id, user_id_mapping, anime_id_mapping, anime_df, top_n=10):\n",
        "        \"\"\"Recommend top N anime for a given user using neural network-based rating prediction\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not loaded. Call load_or_create_model first.\")\n",
        "\n",
        "        if user_id not in user_id_mapping:\n",
        "            print(f\"âŒ User ID {user_id} not found in training data.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        mapped_user_id = user_id_mapping[user_id]\n",
        "        all_anime = np.array(list(anime_id_mapping.values()))\n",
        "\n",
        "        user_array = np.full_like(all_anime, mapped_user_id)\n",
        "        predicted_ratings = self.model.predict([user_array, all_anime], verbose=0)\n",
        "\n",
        "        # Sort anime by predicted rating\n",
        "        top_indices = np.argsort(predicted_ratings[:, 0])[::-1][:top_n]\n",
        "\n",
        "        # Map back to original anime IDs\n",
        "        reverse_anime_mapping = {v: k for k, v in anime_id_mapping.items()}\n",
        "        recommended_anime_ids = [reverse_anime_mapping[all_anime[idx]] for idx in top_indices]\n",
        "\n",
        "        # Return recommendations with ratings\n",
        "        recommendations = anime_df[anime_df['anime_id'].isin(recommended_anime_ids)].copy()\n",
        "\n",
        "        # Add predicted ratings to the recommendations\n",
        "        rating_dict = {reverse_anime_mapping[all_anime[idx]]: predicted_ratings[idx, 0]\n",
        "                      for idx in top_indices}\n",
        "        recommendations['predicted_rating'] = recommendations['anime_id'].map(rating_dict)\n",
        "\n",
        "        # Sort by predicted rating and return relevant columns\n",
        "        recommendations = recommendations.sort_values('predicted_rating', ascending=False)\n",
        "        return recommendations[['name', 'genre', 'predicted_rating']].head(top_n)\n",
        "\n",
        "\n",
        "class GracefulInterruptCallback(tf.keras.callbacks.Callback):\n",
        "    \"\"\"Custom callback to handle graceful interruption during training\"\"\"\n",
        "\n",
        "    def __init__(self, recommendation_system):\n",
        "        super().__init__()\n",
        "        self.rec_system = recommendation_system\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Update current epoch in the recommendation system\n",
        "        self.rec_system.current_epoch = epoch + 1\n",
        "\n",
        "        # Check if training was interrupted\n",
        "        if self.rec_system.training_interrupted:\n",
        "            print(f\"\\nğŸ›‘ Stopping training at epoch {epoch + 1}\")\n",
        "            self.model.stop_training = True\n",
        "\n",
        "\n",
        "# --- Evaluation Metrics (Fixed) ---\n",
        "def precision_at_k(recommended, relevant, k=5):\n",
        "    \"\"\"Calculate precision at k\"\"\"\n",
        "    recommended_top_k = recommended[:k]\n",
        "    relevant_in_top_k = sum([1 for anime in recommended_top_k if anime in relevant])\n",
        "    return relevant_in_top_k / k if k > 0 else 0\n",
        "\n",
        "def recall_at_k(recommended, relevant, k=5):\n",
        "    \"\"\"Calculate recall at k\"\"\"\n",
        "    recommended_top_k = recommended[:k]\n",
        "    relevant_in_top_k = sum([1 for anime in recommended_top_k if anime in relevant])\n",
        "    return relevant_in_top_k / len(relevant) if len(relevant) > 0 else 0\n",
        "\n",
        "def ndcg_at_k(recommended, relevant, k=5):\n",
        "    \"\"\"Calculate NDCG at k\"\"\"\n",
        "    dcg = 0\n",
        "    for i, anime in enumerate(recommended[:k]):\n",
        "        if anime in relevant:\n",
        "            dcg += 1 / np.log2(i + 2)\n",
        "\n",
        "    idcg = sum([1 / np.log2(i + 2) for i in range(min(len(relevant), k))])\n",
        "    return dcg / idcg if idcg > 0 else 0\n",
        "\n",
        "\n",
        "# --- Data Loading and Preprocessing ---\n",
        "def load_and_prepare_data(anime_csv_path, rating_csv_path):\n",
        "    \"\"\"Load and prepare anime and rating data for training\"\"\"\n",
        "    print(\"ğŸ“ Loading data...\")\n",
        "\n",
        "    # Load datasets\n",
        "    anime_df = pd.read_csv(anime_csv_path)\n",
        "    rating_df = pd.read_csv(rating_csv_path)\n",
        "\n",
        "    print(f\"âœ… Loaded {len(anime_df)} anime and {len(rating_df)} ratings\")\n",
        "\n",
        "    # Create ID mappings\n",
        "    anime_id_mapping = {anime_id: idx for idx, anime_id in enumerate(anime_df['anime_id'].unique())}\n",
        "    user_id_mapping = {user_id: idx for idx, user_id in enumerate(rating_df['user_id'].unique())}\n",
        "\n",
        "    print(f\"ğŸ“Š Found {len(user_id_mapping)} unique users and {len(anime_id_mapping)} unique anime\")\n",
        "\n",
        "    # Map IDs to indices\n",
        "    rating_df['anime_id_mapped'] = rating_df['anime_id'].map(anime_id_mapping)\n",
        "    rating_df['user_id_mapped'] = rating_df['user_id'].map(user_id_mapping)\n",
        "\n",
        "    # Clean data - remove rows with invalid mappings\n",
        "    initial_len = len(rating_df)\n",
        "    rating_df = rating_df.dropna(subset=['anime_id_mapped', 'user_id_mapped'])\n",
        "    rating_df = rating_df[(rating_df['anime_id_mapped'] >= 0) & (rating_df['user_id_mapped'] >= 0)]\n",
        "\n",
        "    print(f\"ğŸ§¹ Cleaned data: {initial_len} -> {len(rating_df)} ratings\")\n",
        "\n",
        "    # Prepare features and target\n",
        "    X = rating_df[['user_id_mapped', 'anime_id_mapped']].values\n",
        "    y = rating_df['rating'].values\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    print(f\"ğŸ“ˆ Training set: {len(X_train)} samples\")\n",
        "    print(f\"ğŸ“Š Test set: {len(X_test)} samples\")\n",
        "\n",
        "    return (X_train, X_test, y_train, y_test,\n",
        "            len(user_id_mapping), len(anime_id_mapping),\n",
        "            user_id_mapping, anime_id_mapping, anime_df)"
      ],
      "metadata": {
        "id": "mFXuwQRWp4_F"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Complete example usage of the enhanced recommendation system\"\"\"\n",
        "\n",
        "    # File paths (update these to your actual file paths)\n",
        "    anime_csv_path = '/content/cleaned_anime.csv'\n",
        "    rating_csv_path = '/content/cleaned_rating.csv'\n",
        "\n",
        "    try:\n",
        "        # Load and prepare data\n",
        "        (X_train, X_test, y_train, y_test,\n",
        "         num_users, num_anime,\n",
        "         user_id_mapping, anime_id_mapping, anime_df) = load_and_prepare_data(\n",
        "            anime_csv_path, rating_csv_path\n",
        "        )\n",
        "\n",
        "        # Initialize the recommendation system\n",
        "        rec_system = AnimeRecommendationSystem()\n",
        "\n",
        "        # Load or create model\n",
        "        print(\"ğŸ” Loading or creating model...\")\n",
        "        model_loaded, needs_training = rec_system.load_or_create_model(\n",
        "            num_users, num_anime, embedding_dim=50\n",
        "        )\n",
        "\n",
        "        # Train if needed\n",
        "        if needs_training:\n",
        "            print(\"ğŸ¯ Starting training... (Press Ctrl+C to save and stop)\")\n",
        "            history = rec_system.train_model(\n",
        "                X_train, y_train, X_test, y_test,\n",
        "                epochs=50, batch_size=64\n",
        "            )\n",
        "            print(\"âœ… Training completed!\")\n",
        "        else:\n",
        "            print(\"âœ… Model ready for recommendations!\")\n",
        "\n",
        "        # Make recommendations for a sample user\n",
        "        sample_user_id = list(user_id_mapping.keys())[0]  # Get first user\n",
        "        print(f\"\\nğŸ¬ Getting recommendations for user {sample_user_id}:\")\n",
        "\n",
        "        recommendations = rec_system.recommend_nn_based(\n",
        "            user_id=sample_user_id,\n",
        "            user_id_mapping=user_id_mapping,\n",
        "            anime_id_mapping=anime_id_mapping,\n",
        "            anime_df=anime_df,\n",
        "            top_n=10\n",
        "        )\n",
        "\n",
        "        print(\"\\nğŸ“‹ Top 10 Recommendations:\")\n",
        "        print(recommendations)\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"âŒ File not found: {e}\")\n",
        "        print(\"Please update the file paths in the main() function\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "id": "5hQPT9UWp7Me"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTMcG_Ymp_kt",
        "outputId": "4b440413-79d9-40c8-bc26-45a2742fca08"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“ Loading data...\n",
            "âœ… Loaded 12294 anime and 4708211 ratings\n",
            "ğŸ“Š Found 45003 unique users and 12294 unique anime\n",
            "ğŸ§¹ Cleaned data: 4708211 -> 4708204 ratings\n",
            "ğŸ“ˆ Training set: 3766563 samples\n",
            "ğŸ“Š Test set: 941641 samples\n",
            "ğŸ” Loading or creating model...\n",
            "ğŸ”¨ Creating new model...\n",
            "ğŸ¯ Starting training... (Press Ctrl+C to save and stop)\n",
            "ğŸš€ Starting training from epoch 1/50\n",
            "Epoch 1/50\n",
            "\u001b[1m58853/58853\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8654 - mae: 0.9824\n",
            "Epoch 1: val_loss improved from inf to 1.22916, saving model to model_checkpoint.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m58853/58853\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 3ms/step - loss: 1.8653 - mae: 0.9824 - val_loss: 1.2292 - val_mae: 0.7921\n",
            "Epoch 2/50\n",
            "\u001b[1m58845/58853\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1905 - mae: 0.7824\n",
            "Epoch 2: val_loss improved from 1.22916 to 1.15926, saving model to model_checkpoint.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m58853/58853\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 3ms/step - loss: 1.1905 - mae: 0.7824 - val_loss: 1.1593 - val_mae: 0.7596\n",
            "Epoch 3/50\n",
            "\u001b[1m58841/58853\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1342 - mae: 0.7566\n",
            "Epoch 3: val_loss improved from 1.15926 to 1.14277, saving model to model_checkpoint.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m58853/58853\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 4ms/step - loss: 1.1342 - mae: 0.7566 - val_loss: 1.1428 - val_mae: 0.7578\n",
            "Epoch 4/50\n",
            "\u001b[1m58846/58853\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0979 - mae: 0.7421\n",
            "Epoch 4: val_loss improved from 1.14277 to 1.13884, saving model to model_checkpoint.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m58853/58853\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 4ms/step - loss: 1.0979 - mae: 0.7421 - val_loss: 1.1388 - val_mae: 0.7608\n",
            "Epoch 5/50\n",
            "\u001b[1m58849/58853\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0617 - mae: 0.7276\n",
            "Epoch 5: val_loss improved from 1.13884 to 1.12512, saving model to model_checkpoint.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m58853/58853\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 3ms/step - loss: 1.0617 - mae: 0.7276 - val_loss: 1.1251 - val_mae: 0.7535\n",
            "Epoch 6/50\n",
            "\u001b[1m58851/58853\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0304 - mae: 0.7152\n",
            "Epoch 6: val_loss improved from 1.12512 to 1.11645, saving model to model_checkpoint.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m58853/58853\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 3ms/step - loss: 1.0304 - mae: 0.7152 - val_loss: 1.1165 - val_mae: 0.7505\n",
            "Epoch 7/50\n",
            "\u001b[1m58843/58853\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0036 - mae: 0.7047\n",
            "Epoch 7: val_loss improved from 1.11645 to 1.10614, saving model to model_checkpoint.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m58853/58853\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 3ms/step - loss: 1.0036 - mae: 0.7047 - val_loss: 1.1061 - val_mae: 0.7408\n",
            "Epoch 8/50\n",
            "\u001b[1m58845/58853\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9755 - mae: 0.6937\n",
            "Epoch 8: val_loss did not improve from 1.10614\n",
            "\u001b[1m58853/58853\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 3ms/step - loss: 0.9755 - mae: 0.6937 - val_loss: 1.1074 - val_mae: 0.7420\n",
            "Epoch 9/50\n",
            "\u001b[1m58853/58853\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9539 - mae: 0.6851\n",
            "Epoch 9: val_loss did not improve from 1.10614\n",
            "\u001b[1m58853/58853\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m207s\u001b[0m 4ms/step - loss: 0.9539 - mae: 0.6851 - val_loss: 1.1161 - val_mae: 0.7417\n",
            "Epoch 10/50\n",
            "\u001b[1m58848/58853\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9320 - mae: 0.6766\n",
            "Epoch 10: val_loss did not improve from 1.10614\n",
            "\u001b[1m58853/58853\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 4ms/step - loss: 0.9320 - mae: 0.6766 - val_loss: 1.1077 - val_mae: 0.7411\n",
            "Epoch 11/50\n",
            "\u001b[1m58847/58853\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9161 - mae: 0.6702\n",
            "Epoch 11: val_loss did not improve from 1.10614\n",
            "\u001b[1m58853/58853\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 3ms/step - loss: 0.9161 - mae: 0.6702 - val_loss: 1.1271 - val_mae: 0.7474\n",
            "Epoch 12/50\n",
            "\u001b[1m58851/58853\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8955 - mae: 0.6627\n",
            "Epoch 12: val_loss did not improve from 1.10614\n",
            "\u001b[1m58853/58853\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 3ms/step - loss: 0.8955 - mae: 0.6627 - val_loss: 1.1086 - val_mae: 0.7427\n",
            "Epoch 12: early stopping\n",
            "Restoring model weights from the end of the best epoch: 7.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Training completed! Model saved to anime_recommendation_model.h5\n",
            "ğŸ§¹ Cleaned up checkpoint: model_checkpoint.h5\n",
            "âœ… Training completed!\n",
            "\n",
            "ğŸ¬ Getting recommendations for user 1:\n",
            "\n",
            "ğŸ“‹ Top 10 Recommendations:\n",
            "                                           name  \\\n",
            "0                                Kimi no Na wa.   \n",
            "255                           Fairy Tail (2014)   \n",
            "804                            Sword Art Online   \n",
            "686               Bishoujo Senshi Sailor Moon S   \n",
            "884    Bishoujo Senshi Sailor Moon R: The Movie   \n",
            "534   Bishoujo Senshi Sailor Moon: Sailor Stars   \n",
            "1177         Bishoujo Senshi Sailor Moon SuperS   \n",
            "876                               Ranma Â½ Super   \n",
            "967               Bishoujo Senshi Sailor Moon R   \n",
            "1068                Ranma Â½: Akumu! Shunmin Kou   \n",
            "\n",
            "                                                  genre  predicted_rating  \n",
            "0                  Drama, Romance, School, Supernatural          8.522452  \n",
            "255   Action, Adventure, Comedy, Fantasy, Magic, Sho...          8.295575  \n",
            "804           Action, Adventure, Fantasy, Game, Romance          8.266942  \n",
            "686                       Drama, Magic, Romance, Shoujo          8.264772  \n",
            "884                       Drama, Magic, Romance, Shoujo          8.251074  \n",
            "534   Adventure, Comedy, Drama, Fantasy, Magic, Roma...          8.240087  \n",
            "1177                      Drama, Magic, Romance, Shoujo          8.238618  \n",
            "876   Adventure, Comedy, Martial Arts, Romance, Shou...          8.221959  \n",
            "967                      Demons, Magic, Romance, Shoujo          8.221050  \n",
            "1068  Action, Comedy, Martial Arts, Shounen, Superna...          8.216516  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "pickle.dump(user_id_mapping, open('user_id_mapping.pkl', 'wb'))\n",
        "pickle.dump(anime_id_mapping, open('anime_id_mapping.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "KuS53VZazyZb"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}